id: bargaining-with-the-future
name: Bargaining with the Future
description: Automation, Agency, and the Design of Personal AI Systems
problem_statement: >
  Digital interaction demands constant cognitive labor: selecting tools, managing fragmented context,
  navigating rapid transitions. This produces digital fatigue—not from technology use itself, but
  from the overhead of operating technology.
context: >
  The app-centric paradigm forces users to maintain context across fragmented tools. AI assistance
  promises relief but creates new friction: systems that optimize on users' behalf often feel
  alien. Users report feeling disconnected from decisions made in their name—a loss of agency that
  undermines the automation benefit.
research_question: >
  What interaction patterns enable AI systems to reduce cognitive overhead while maintaining user
  agency? Can automation boundaries, override mechanisms, and value alignment be made structural
  rather than optional?
approach: >
  This thesis explores these questions through LifeOS—a speculative mode-intent architecture where:


  1. Context determines available actions (modes)

  2. Users select from system-surfaced options (intents)

  3. Decisions align with articulated values (constitutional frameworks)


  The architecture makes a specific bet: that explicit automation boundaries and structured override
  paths can preserve agency while delivering convenience. Future self-continuity research suggests
  this bet may work—when users feel connected to the "future self" the system optimizes for,
  proactive assistance feels helpful rather than imposing.
core_tension: >
  Every design decision negotiates the same tradeoff: present autonomy vs. future benefit. When
  should the system act as "future you's advocate"? How much override friction maintains agency
  without recreating cognitive overhead? These questions have no universal answers—only
  context-dependent negotiations.
scope_and_limitations: >
  Due to the scope of such a problem, this thesis likely won't be able to solve nor pretend to
  propose a solution to this tension. 


  1. The tension is real, persistent, and underexplored — Neither tech-optimist framing (automation
  is unambiguously good) nor tech-critical framing (automation necessarily erodes agency) provides
  useful guidance for practitioners


  2. Mode-intent provides structural vocabulary for engaging with it — Rather than abstract debates
  about "AI and autonomy," designers can examine specific questions: How should mode confidence
  thresholds work? What makes intent surfacing feel helpful vs. manipulative? Where should override
  friction sit?


  3. LifeOS serves as reference architecture, not prescription — A sufficiently specified
  speculative system that enables concrete debate about design decisions, rather than speculation
  from scratch


  The contribution is a platform for informed engagement with this tension—not a claim to have
  solved it.
contributions:
  - id: mode-intent-architecture
    name: Mode-intent as interaction architecture
    description: >
      A structural framework for context-adaptive, intent-driven systems where:

      - Modes define contextual solution space (system-managed)

      - Intents represent user goals within that space (user-selected from surfaced options)

      - Generative UI constructs purpose-built interfaces to serve selected intents


      This architecture proposes that the application layer becomes obsolete when the system can
      determine context, surface relevant intents, and generate interfaces to serve them.
  - id: lifeos-artifact
    name: LifeOS as speculative design artifact
    description: >
      A fully-realized reference system that:

      - Demonstrates mode-intent architecture in concrete detail

      - Makes the automation-agency tension visible and debatable at the level of specific design
      decisions

      - Provides practitioners a platform for exploring human-AI interaction paradigms without
      starting from zero
